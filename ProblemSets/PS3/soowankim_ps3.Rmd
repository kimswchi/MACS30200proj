---
title: 'Problem Set #3: Regression Diagnostics, Interaction Terms, and Missing Data'
author: "Soo Wan Kim"
date: "May 11, 2017"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
library(broom)
library(lmtest)
library(car)

set.seed(1234)
theme_set(theme_minimal())
setwd("~/GitHub/MACS30200proj/ProblemSets/PS3")


biden <- read.csv("data/biden.csv")
```

## Regression diagnostics

```{r}
biden1 <- biden %>%
  na.omit()

lr_mod1 <- lm(biden ~ age + female + educ, data = biden1)
tidy(lr_mod1)
```

### Part 1
**Test the model to identify any unusual and/or influential observations. Identify how you would treat these observations moving forward with this research. Note you do not actually have to estimate a new model, just explain what you would do. This could include things like dropping observations, respecifying the model, or collecting additional variables to control for this influential effect.**

```{r bubble_plot}
# add key statistics
bid1_augment <- biden1 %>%
  mutate(hat = hatvalues(lr_mod1),
         student = rstudent(lr_mod1),
         cooksd = cooks.distance(lr_mod1))

# draw bubble plot
ggplot(bid1_augment, aes(hat, student)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(size = cooksd), shape = 1) +
  geom_text(data = bid1_augment %>%
              arrange(-cooksd) %>%
              slice(1:10),
            aes(label = biden)) +
  scale_size_continuous(range = c(1, 20)) +
  labs(x = "Leverage",
       y = "Studentized residual") +
  theme(legend.position = "none")
```

The bubble plot shows many observations with high discrepancy and some observations with high leverage as well as high discrepancy.

```{r outlier_numerical_rules_of_thumb}
bid1_hat <- bid1_augment %>%
  filter(hat > 2 * mean(hat))
nrow(bid1_hat)

bid1_stud <- bid1_augment %>%
  filter(abs(student) > 2)

bid1_inf <- bid1_augment %>%
  filter(cooksd > 4 / (nrow(.) - (length(coef(lr_mod1)) - 1) - 1))
```

Using the hat-values test, I find `nrow(bid1_hat)` observations with unusually high leverage. Using the studentized residuals test, I find `nrow(bid1_stud)` highly discrepant observations. Using the influence test, I find `nrow(bid1_inf)` highly influential observations. Looking through the observations, I do not find any that are obviously miscoded. The high discrepancy observations have unusually low values for `biden`, whereas the high leverage observations have unusually high values for `age` and unusually low values for `educ`'. 

```{r variable_density_plots}
#Biden thermometer
ggplot(biden1, aes(biden)) + 
  geom_density() + 
  geom_density(data = bid1_hat, mapping = aes(biden), color = "red") + 
  geom_density(data = bid1_stud, mapping = aes(biden), color = "blue") + 
  geom_density(data = bid1_inf, mapping = aes(biden), color = "green") + 
  labs(title = "Density plot of Biden feeling thermometer measures",
       x = "Biden feeling thermometer reading",
       y = "Density",
       caption = "Black: Entire set, NA values omitted\n
       Red: High leverage observations\n
       Blue: High discrepancy observerations\n
       Green: High influence observations")

#Age
ggplot(biden1, aes(age)) + 
  geom_density() + 
  geom_density(data = bid1_hat, mapping = aes(age), color = "red") + 
  geom_density(data = bid1_stud, mapping = aes(age), color = "blue") + 
  geom_density(data = bid1_inf, mapping = aes(age), color = "green") + 
  labs(title = "Density plot of age",
       x = "Age",
       y = "Density",
       caption = "Black: Entire set, NA values omitted\n
       Red: High leverage observations\n
       Blue: High discrepancy observerations\n
       Green: High influence observations")

#Gender
ggplot(biden1, aes(female)) + 
  geom_density() + 
  geom_density(data = bid1_hat, mapping = aes(female), color = "red") + 
  geom_density(data = bid1_stud, mapping = aes(female), color = "blue") + 
  geom_density(data = bid1_inf, mapping = aes(female), color = "green") + 
  labs(title = "Density plot of gender",
       x = "Gender (0 = Male, 1 = Female)",
       y = "Frequency",
       caption = "Black: Entire set, NA values omitted\n
       Red: High leverage observations\n
       Blue: High discrepancy observerations\n
       Green: High influence observations")

#Education
ggplot(biden1, aes(educ)) + 
  geom_density() + 
  geom_density(data = bid1_hat, mapping = aes(educ), color = "red") + 
  geom_density(data = bid1_stud, mapping = aes(educ), color = "blue") + 
  geom_density(data = bid1_inf, mapping = aes(educ), color = "green") + 
  labs(title = "Density plot of education level",
       x = "Years of schooling",
       y = "Density",
       caption = "Black: Entire set, NA values omitted\n
       Red: High leverage observations\n
       Blue: High discrepancy observerations\n
       Green: High influence observations")
```


Since the only weird thing about these observations are the extreme values and there is nothing to suggest that these are caused by measurement error, miscoding, or anything else that invalidates the observations, I would keep them in the dataset. Instead of dropping the unusual observations I would try to mitigate error by respecifying the model. Specifically, I would add party identification as another independent variable because I suspect party identification is an important determinant of attitudes toward Biden. I could also transform the variables to reduce skewness and variance by taking logs, power transformations, etc. 

### Part 2
**Test for non-normally distributed errors. If they are not normally distributed, propose how to correct for them.**

```{r error_distribution_plots}
car::qqPlot(lr_mod1)

augment(lr_mod1, biden1) %>%
  mutate(.student = rstudent(lr_mod1)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density")
```

The quantile comparison plot and the density plot of studentized residuals indicate that the errors are not normally distributed. To correct for this, I would take log or power transformations of variables to make their values more normally distributed.

### Part 3 
**Test for heteroscedasticity in the model. If present, explain what impact this could have on inference.**

```{r breusch_pagan_test}
bptest(lr_mod1)
```

The result of the Breusch-Pagan test indicates that heteroscedasticity is present.

```{r heteroscedasticity_graphical_test}
biden1 %>%
  add_predictions(lr_mod1) %>%
  add_residuals(lr_mod1) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

From graphing the residuals against predicted values, it looks like residuals become more negative as predicted values increase. This suggests the model systematically overestimates or underestimates Biden warmth based on combinations of independent variable values.

### Part 4
**Test for multicollinearity. If present, propose if/how to solve the problem.**

```{r vif_test}
vif(lr_mod1)
```

The VIF statistic is less than 10 for all variables, which indicates multicollinearity is not present. 

### Interaction terms

```{r}
biden2 <- biden %>%
  na.omit()

lr_mod2 <- lm(biden ~ age*educ, data = biden2)
tidy(lr_mod2)
```

#### Part 1
Evaluate the marginal effect of age on Joe Biden thermometer rating, conditional on education. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.

#### Part 2
Evaluate the marginal effect of education on Joe Biden thermometer rating, conditional on age. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.

```{r}

```

### Missing data

Estimate the following linear regression model of attitudes towards Joseph Biden:



where Y is the Joe Biden feeling thermometer, X1 is age, X2 is gender, and X3 is education. This time, use multiple imputation to account for the missingness in the data. Consider the multivariate normality assumption and transform any variables as you see fit for the imputation stage. Calculate appropriate estimates of the parameters and the standard errors and explain how the results differ from the original, non-imputed model.

```{r}

```